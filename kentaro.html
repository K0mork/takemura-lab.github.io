<li>令和5(2023)年4月1日 東海大学 情報技術センター 次長</li><!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dr. Kentaro Takemura | Human Sensing Group</title>
    <link rel="stylesheet" href="./tailwind.min.css">
</head>
<body class="bg-white-100 text-gray-900">
    <!-- メニューを読み込む場所 -->
    <div id="header-placeholder"></div>
    <main class="container mx-auto px-6 py-10">
        <section>
            <h2 class="text-2xl font-bold mb-4">Dr. Kentaro Takemura</h2>
            <section class="text-center mb-6">
                <img src="kentaro.webp" alt="Dr. Kentaro Takemura" class="mx-auto rounded-full w-32 h-32">
            </section>
            <p class="mb-4">
                Kentaro Takemura is a Professor of the Graduate School of Science and Technology, Graduate School of Engineering, and School of Information Science and Technology at Tokai University. Additionally, he is Deputy Director at Tokai University Research & Information Center (TRIC). He joined Tokai University in 2013 and founded the Human Sensing Group as a director.
            </p>
            <p class="mb-4">
                He always seeks state-of-the-art in his research field, so he studies international collaborative research aggressively. He worked on smart cars at Australian National University in 2005, researched first-person vision in the Robotics Institute at Carnegie Mellon University as a visiting researcher from 2011-2012, and bio-robotics at Georgia Tech as a visiting scholar in 2018.
            </p>
            <p class="mb-4">
                Before Tokai University, from 2008-2013, he was an Assistant Professor at Nara Institute of Science and Technology. From 2006 – 2008, he was an Assistant Professor at Nara National College of Technology.
            </p>
            <p class="mb-4">
                He received his B.E. degree in electrical engineering from Shibaura Institute of Technology in 2001, and M.E. and Ph.D. degrees in information science from Nara Institute of Science and Technology, Japan, in 2003 and 2006, respectively.
            </p>

            <h3 class="text-xl font-semibold mt-6 mb-2 text-red-600">Education</h2>
            <ul class="list-disc pl-6 mb-4">
                <li>平成9(1997)年3月 私立 帝京高等学校 卒業</li>
                <li>平成9(1997)年4月 芝浦工業大学工学部 電気工学科 入学</li>
                <li>平成13(2001)年3月 同上 卒業 (2001年3月19日)</li>
                <li>平成13(2001)年4月 奈良先端科学技術大学院大学情報科学研究科情報システム学専攻博士前期課程 入学</li>
                <li>平成15(2003)年3月 同上 修了(2003年3月24日)</li>
                <li>平成15(2003)年4月 奈良先端科学技術大学院大学 情報科学研究科情報システム学専攻博士後期課程 入学</li>
                <li>平成18(2006)年3月 同上 修了 博士(工学) (2006年3月24日)</li>
            </ul>

            <h3 class="text-xl font-semibold mt-6 mb-2 text-red-600">Work experience</h3>
            <ul class="list-disc pl-6 mb-4">
                <li>平成16(2004)年4月1日 ユビキタス統合メディアコンピューティング COE奨励研究員 (2006年3月31日まで)</li>
                <li>平成16(2004)年10月1日 株式会社国際電気通信基礎技術研究所 メディア情報科学研究所 研修研究員(2005年3月31日まで)</li>
                <li>平成18(2006)年4月1日 独立行政法人 国立高等専門学校機構 奈良工業高等専門学校 電気工学科 助手</li>
                <li>平成19(2007)年4月1日 独立行政法人 国立高等専門学校機構 奈良工業高等専門学校 電気工学科 助教</li>
                <li>平成20(2008)年4月1日 国立大学法人 奈良先端科学技術大学院大学 情報科学研究科 助教</li>
                <li>平成23(2011)年3月1日 Carnegie Mellon University, The Robotics Institute, Visiting Researcher （2012年2月29日まで）</li>
                <li>平成25(2013)年4月1日 東海大学 情報理工学部 コンピュータ応用工学科 講師</li>
                <li>平成27(2015)年4月1日 東海大学 情報理工学部 コンピュータ応用工学科 准教授</li>
                <li>平成30(2018)年3月1日 Georgia Institute of Technology, George W. Woodruff School of Mechanical Engineering, Visiting Scholar （2018年9月30日まで）</li>
                <li>令和2(2020)年4月1日 東海大学 情報理工学部 コンピュータ応用工学科 教授</li>
                <li>令和3(2021)年4月1日 東海大学 情報技術センター 兼任</li>
                <li>令和5(2023)年4月1日 東海大学 情報技術センター 次長</li>
                <li>令和7(2025)年4月1日 東海大学 情報理工学部 コンピュータ応用工学科 学科長</li>

            </ul>

            <h3 class="text-xl font-semibold mt-6 mb-2 text-red-600">Teaching Activities<h3>
            <ul class="list-disc pl-6 mb-4">
                <li>東海大学 線形代数, 2022-</li>
                <li>東海大学 コンピュータビジョン, 2020-</li>
                <li>東海大学 コンピュータグラフィックス, 2021-</li>
                <li>東海大学 電子回路, 2013-</li>
                <li>東海大学大学院 ロボットビジョン特論, 2017-</li>
            </ul>

            <h3 class="text-xl font-semibold mt-6 mb-2 text-red-600">Academic Society</h3>
            <ul class="list-disc pl-6 mb-4">
                <li>2005年12月1日 ACM 会員</li>
                <li>2008年6月23日 IEEE 会員</li>
                <li>2002年7月18日 日本ロボット学会 会員</li>
                <li>画像電子学会 会員</li>
                <li>計測自動制御学会 会員</li>
            </ul>

            <h3 class="text-xl font-semibold mt-6 mb-2 text-red-600">Competitive funds</h3>
            <ul class="list-disc pl-6 mb-4">
                <li>科研費 基盤(B)，スクリーン上へ提示した不知覚なマーカによる能動的なイベントベース視線計測技術，R6-R9 (代表)</li>
                <li>科研費 挑戦的研究(開拓)，視線の文法化による脳機能・脳内病理の推定手法，R4-R7 (分担)</li>
                <li>科研費 挑戦的研究(萌芽)，角膜上の環境反射像を用いた位置推定，R3-R4 (代表)</li>
                <li>科研費 基盤(B)，偏光を用いた視線計測技術の確立と多様なディスプレイ環境への応用，R3-R5 (代表)</li>
                <li>科研費 基盤(B)，幾何学的な制約、画面情報、瞳孔径の変化を用いた校正不要な視線計測技術，R2-R5 (分担)</li>
                <li>科研費 基盤(B)，角膜表面上のディスプレイ反射像を用いた可視光視線計測技術，H30-H32 (代表)</li>
                <li>科研費 挑戦的研究(萌芽)，体導音を用いた身体情報センシング，H30-H31 (代表)</li>
                <li>総務省SCOPE若手ICT研究者等育成型研究開発，アクティブ骨導音センシングを用いた次世代インタフェース技術の研究開発，H28-H30 (代表)</li>
                <li>科研費 基盤(B)，高精度ランタイムキャリブレーション視線計測手法とその応用，H28-H31 (分担)</li>
                <li>科研費 国際共同研究加速基金(国際共同研究強化)，触覚情報の記録・提示が可能なアクティブ骨導音センシング (交付内定)</li>
                <li>科研費 基盤(C)，舞踊動作を規範とした「優美さ」特徴のモデル化 -動きとフォルムの両側面から迫る-，H29-H31 (分担)</li>
                <li>科研費 基盤(C)，触覚情報の記録・提示が可能なアクティブ骨導音センシング，H27-H29 (代表)</li>
                <li>科研費 基盤(C)，動作における「優美さ」評価方法の確立 -「美の線」を基準とした計測・評価-，H26-H28 (分担)</li>
                <li>科研費 基盤(B)，衛星観測と現地調査による被災地の環境再生モニタリングと地球環境教育の実践，H27-H28 (分担)</li>
                <li>JST 戦略的国際科学技術協力推進事業欧州諸国との研究交流（CONCERT-Japan），H25-H26 (分担)</li>
                <li>奈良先端大未来開拓コロキウム，「家政学とロボティクスの融合による生活ロボティクスへの展開」，H24年度，100万円 (代表)</li>
                <li>総務省 戦略的情報通信研究開発推進制度(SCOPE)，若手ICT研究者等育成型研究開発「角膜表面反射画像を用いた注視点・注視対象推定の研究開発」，H24-H26 (予定) (代表)</li>
                <li>科研費 基盤(C)，アクティブ骨導音センシングを用いた常時装着型手入力インタフェース，H24-H26 (代表)</li>
                <li>科研費 基盤(B)，多次元行動情報を付加したセマンティックマップによるロボットサービス，H24-H26 (分担)</li>
                <li>科研費 基盤(C)，「優美な」動作のモデル化とロボット「優美」動作生成法の確立，H23-H25 (分担)</li>
                <li>科研費 新学術領域，ヒューマンロボットインタラクションのための動作データベースに基づく柔軟な動作生成，H22-H23 (代表)</li>
                <li>科研費 若手研究(B)，自由頭部運動状況下における三次元環境地図に基づく注視点推定，H22-H23 (代表)</li>
                <li>科研費 若手研究(B)，拡張現実感における注視駆動型情報提示技術の開発，H20-H22 (代表)</li>
                <li>科研費 萌芽研究，古典舞踊に倣う対話ロボット上半身の身振り生成法の提案，H20-H22 (分担)</li>
                <li>NEDO技術開発機構，次世代ロボット知能化技術開発プロジェクト 移動・作業知能のための視覚に基づくロバストな知能モジュールの開発，H19-H23 (分担)</li>
                <li>電気通信普及財団，平成19年度海外渡航旅費援助金</li>
                <li>NEDO技術開発機構，戦略的先端ロボット要素技術開発プロジェクト 高齢者対応コミュニケーションRTシステム，H18-H20 (分担)</li>
                <li>科研費 基盤(B)，生体信号モニタリングと統合ヒューマンモデルによる身体運動時の筋力設計，H18-H20 (分担)</li>
                <li>科研費 基盤(B)，注視情報の伝達に基づくコミュニケーション支援空間の構築，H16-H18 (分担)</li>
                <li>2005年度 奈良先端大 情報科学研究科 COE研究助成金 若手研究者育成事業</li>
                <li>2004年度 奈良先端大 情報科学研究科 COE研究助成金 若手研究者育成事業</li>
                <li>2001年度 奈良先端大 情報科学研究科 研究育成基金</li>
            </ul>

            <h3 class="text-xl font-semibold mt-6 mb-2 text-red-600">Awards</h3>
            <ul class="list-disc pl-6 mb-4">
                <li>令和元年10月9日 総務省 戦略的情報通信研究開発推進事業事務局 研究開発奨励賞 竹村憲太郎</li>
                <li>平成26年9月5日 日本ロボット学会 第29回研究奨励賞 受賞</li>
                <li>平成13年3月19日 芝浦工業大学 創立者 有元史郎 記念賞 受賞</li>
            </ul>
        </section>
    </main>

    <footer class="bg-gray-800 text-white text-center py-4 mt-10">
        <p>&copy; Human Sensing Group 2025</p>
    </footer>
    <script>
        // JavaScriptでheader.htmlを読み込む
        fetch('header.html')
            .then(response => {
                if (!response.ok) {
                    throw new Error('Network response was not ok');
                }
                return response.text();
            })
            .then(data => {
                document.getElementById('header-placeholder').innerHTML = data;

                // ハンバーガーメニューの開閉を制御
                const menuToggle = document.getElementById('menu-toggle');
                const mobileMenu = document.getElementById('mobile-menu');
                if (menuToggle && mobileMenu) {
                    menuToggle.addEventListener('click', () => {
                        mobileMenu.classList.toggle('hidden');
                    });
                }
            })
            .catch(error => console.error('Error loading header:', error));
    </script>
</body>
</html>
